{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcOlgyrIfaR8",
        "outputId": "90d667a8-4302-4648-e7ed-a488fe85a340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym) (0.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# === Install Packages ===\n",
        "!pip install torch gym\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import imageio\n",
        "import io\n",
        "from collections import deque\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KEjXl_Uoa4aJ"
      },
      "outputs": [],
      "source": [
        "# === Define the Environment ===\n",
        "class ColorTilesEnv(gym.Env):\n",
        "    def __init__(self, size=4, target_scores=None, inks=None, player_start=None, gamma=0.99):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.board_size = size + 2  # include sentinels\n",
        "        self.target_scores = target_scores\n",
        "        self.inks = inks\n",
        "        self.player_start = player_start\n",
        "        self.gamma = gamma\n",
        "        self.action_space = spaces.Discrete(4)  # 0:up,1:right,2:down,3:left\n",
        "        self.observation_space = spaces.Box(0, 33, shape=(self.board_size,self.board_size), dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = np.zeros((self.board_size,self.board_size),dtype=int)\n",
        "        for y in range(self.board_size):\n",
        "            for x in range(self.board_size):\n",
        "                if x==0 or x==self.size+1 or y==0 or y==self.size+1:\n",
        "                    self.data[y][x] = 6 # sentinels\n",
        "        for (y,x), v in self.inks.items():\n",
        "            self.data[y][x] = v\n",
        "        self.px, self.py = self.player_start\n",
        "        self.player_color = 0\n",
        "        self.prev_color = 0\n",
        "        self.step_count = 0\n",
        "        self.color_scores = {'red':0,'blue':0,'green':0}\n",
        "\n",
        "        # save 3 recent actions\n",
        "        self.action_history = deque(maxlen=4)\n",
        "\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        # return normalized info\n",
        "        return np.copy(self.data) / 33.0\n",
        "\n",
        "    # Potential Function\n",
        "    def _potential(self):\n",
        "        remaining = 0\n",
        "        for y in range(1, self.size+1):\n",
        "            for x in range(1, self.size+1):\n",
        "                if self.data[y][x] == 0:\n",
        "                    remaining += 1\n",
        "        # The fewer tiles remaining, the higher the potential\n",
        "        return -remaining / (self.size * self.size)\n",
        "\n",
        "    def step(self, action):\n",
        "        old_potential = self._potential()\n",
        "\n",
        "        directions = [(0,-1),(1,0),(0,1),(-1,0)]\n",
        "        dx, dy = directions[action]\n",
        "        nx, ny = self.px + dx, self.py + dy\n",
        "        cell = self.data[ny][nx]\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "        self.prev_color = self.player_color\n",
        "\n",
        "        if cell == 11: self.player_color = 1\n",
        "        elif cell == 22: self.player_color = 2\n",
        "        elif cell == 33: self.player_color = 3\n",
        "\n",
        "        # cannot move toward a wall or the other color\n",
        "        if cell == 6 or (cell > 0 and cell not in (11,22,33) and cell != self.player_color):\n",
        "            return None # ignore invalid move\n",
        "\n",
        "        # punish invalid moves\n",
        "        self.action_history.append(action)\n",
        "        if len(self.action_history) >= 3:\n",
        "            a1, a2, a3 = list(self.action_history)[-3:]\n",
        "            opposite = {0:2, 1:3, 2:0, 3:1}\n",
        "            if a1 == a3 and a2 == opposite[a1]:\n",
        "                reward -= 4  # penalty\n",
        "\n",
        "        color_name = {1:'red',2:'blue',3:'green'}.get(self.player_color)\n",
        "        if self.player_color > 0:\n",
        "            if cell != self.player_color:\n",
        "                self.data[ny][nx] = self.player_color\n",
        "                reward += 1 # fill a tile\n",
        "                self.color_scores[color_name] += 1\n",
        "                if self.color_scores[color_name] == self.target_scores[color_name]:\n",
        "                    reward += 2 # achieve a target_score\n",
        "            else:\n",
        "                reward -= 2  # move toward the same color tile\n",
        "\n",
        "        # get an ink after achieving a target_score\n",
        "        if (self.prev_color != 0 and self.prev_color != self.player_color):\n",
        "            prev_color_name = {1:'red',2:'blue',3:'green'}.get(self.prev_color)\n",
        "            if self.color_scores[prev_color_name] == self.target_scores[prev_color_name] and self.prev_color != self.player_color:\n",
        "                reward += 5\n",
        "\n",
        "        # movement\n",
        "        self.px, self.py = nx, ny\n",
        "        self.step_count += 1\n",
        "\n",
        "        # avoid infinite loops\n",
        "        if self.step_count >= 200:\n",
        "            done = True\n",
        "\n",
        "        # judgement\n",
        "        if self._all_tiles_colored():\n",
        "            reward += 20  # Game Clear bonus\n",
        "            done = True\n",
        "        elif not self._can_fill_more() or not self._is_color_enough() or \\\n",
        "              self.color_scores['red'] > self.target_scores['red'] or \\\n",
        "              self.color_scores['blue'] > self.target_scores['blue'] or \\\n",
        "              self.color_scores['green'] > self.target_scores['green']:\n",
        "            reward -= 5  # Game Over\n",
        "            done = True\n",
        "\n",
        "        # Potential Shaping\n",
        "        new_potential = self._potential()\n",
        "        shaping_reward = self.gamma * new_potential - old_potential\n",
        "        reward += shaping_reward\n",
        "\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "    def _all_tiles_colored(self):\n",
        "        for y in range(1,self.size+1):\n",
        "            for x in range(1,self.size+1):\n",
        "                if self.data[y][x] in (0, 11, 22, 33):\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def _can_fill_more(self):\n",
        "        directions = [(0,-1),(1,0),(0,1),(-1,0)]\n",
        "        for y in range(1,self.size+1):\n",
        "            for x in range(1,self.size+1):\n",
        "                if self.data[y][x] != self.player_color:\n",
        "                    continue\n",
        "                for dx, dy in directions:\n",
        "                    nx, ny = x+dx, y+dy\n",
        "                    if self.data[ny][nx] in (0,11,22,33):\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "    def _is_color_enough(self):\n",
        "        if self.color_scores['red'] >= 1 and self.color_scores['red'] < self.target_scores['red'] and self.player_color != 1:\n",
        "            return False\n",
        "        if self.color_scores['blue'] >= 1 and self.color_scores['blue'] < self.target_scores['blue'] and self.player_color != 2:\n",
        "            return False\n",
        "        if self.color_scores['green'] >= 1 and self.color_scores['green'] < self.target_scores['green'] and self.player_color != 3:\n",
        "            return False\n",
        "        return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "a7_FFLNAMT30",
        "outputId": "bc4d8b11-42c3-401d-e138-2fa557fe501f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Score: (Red: 10, Blue: 2, Green: 4)\n"
          ]
        }
      ],
      "source": [
        "# === Draw Initial Board ===\n",
        "def draw_board(data, px, py, save_fig=False):\n",
        "    if not save_fig:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(SIZE, SIZE))\n",
        "    ax.set_xlim(0, SIZE)\n",
        "    ax.set_ylim(0, SIZE)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_aspect(\"equal\")\n",
        "\n",
        "    # Grid\n",
        "    for x in range(SIZE + 1):\n",
        "        ax.plot([x, x], [0, SIZE], color=\"black\")\n",
        "    for y in range(SIZE + 1):\n",
        "        ax.plot([0, SIZE], [y, y], color=\"black\")\n",
        "\n",
        "    # Each Tile\n",
        "    for y in range(1, SIZE+1):\n",
        "        for x in range(1, SIZE+1):\n",
        "            val = data[y][x]\n",
        "            if val == 1 or val == 11:\n",
        "                color = \"#ff5252\"  # red\n",
        "            elif val == 2 or val == 22:\n",
        "                color = \"#7192f5\"  # blue\n",
        "            elif val == 3 or val == 33:\n",
        "                color = \"#30cf8a\"  # green\n",
        "            else:\n",
        "                color = \"white\"\n",
        "            if val in (1, 2, 3): # filled\n",
        "                ax.add_patch(plt.Rectangle((x-1, SIZE-y), 1, 1, color=color))\n",
        "            elif val in (11, 22, 33): # inks\n",
        "                ax.add_patch(plt.Circle((x-0.5, SIZE-y+0.5), 0.2, color=color))\n",
        "\n",
        "    # Player\n",
        "    ax.plot([px-0.5], [SIZE-py+0.5], marker=\"s\", color='black', markerfacecolor=\"none\", markersize=20)\n",
        "\n",
        "    if save_fig:\n",
        "        return fig # for gif\n",
        "    else:\n",
        "        plt.show() # animation\n",
        "\n",
        "\n",
        "# === Define the Board ===\n",
        "SIZE = 4\n",
        "\n",
        "target_scores = {\n",
        "    'red': 10,\n",
        "    'blue': 2,\n",
        "    'green': 4\n",
        "}\n",
        "\n",
        "inks = {\n",
        "    (2, 2): 11,  # red\n",
        "    (2, 4): 22,  # blue\n",
        "    (3, 3): 33   # green\n",
        "}\n",
        "\n",
        "player_start = (2, 3)\n",
        "\n",
        "env = ColorTilesEnv(\n",
        "    size=SIZE,\n",
        "    inks=inks,\n",
        "    target_scores=target_scores,\n",
        "    player_start=player_start\n",
        ")\n",
        "state = env.reset()\n",
        "\n",
        "# Draw\n",
        "draw_board(env.data, env.px, env.py)\n",
        "print(f\"Target Score: (Red: {target_scores['red']}, Blue: {target_scores['blue']}, Green: {target_scores['green']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Re2DcWZ6fSJl"
      },
      "outputs": [],
      "source": [
        "def remove_redundant_moves(transitions=None, action_history=None, state_history=None, px=None, py=None):\n",
        "    directions = [(0,-1),(1,0),(0,1),(-1,0)]\n",
        "    position_history = [(px, py)] # player pos\n",
        "    i = 0\n",
        "\n",
        "    if transitions is not None:\n",
        "        while i < len(transitions)-1:\n",
        "            s, a, r, s_next, done = transitions[i]\n",
        "            dx, dy = directions[a]\n",
        "            px += dx\n",
        "            py += dy\n",
        "            position_history.append((px, py))\n",
        "\n",
        "            # search backward for the same pos\n",
        "            for j in range(len(position_history) - 2, -1, -1):\n",
        "                if position_history[j] == (px, py) and np.array_equal(transitions[j][0], s_next):\n",
        "                      del transitions[j:i+1]\n",
        "                      del position_history[j:i+1]\n",
        "                      i = j-1\n",
        "                      break\n",
        "            i += 1\n",
        "\n",
        "        return transitions\n",
        "\n",
        "    elif action_history is not None and state_history is not None:\n",
        "        while i < len(action_history)-1:\n",
        "            dx, dy = directions[action_history[i]]\n",
        "            px += dx\n",
        "            py += dy\n",
        "            position_history.append((px, py))\n",
        "\n",
        "            for j in range(len(position_history) - 2, -1, -1):\n",
        "                if position_history[j] == (px, py) and np.array_equal(state_history[j], state_history[i+1]):\n",
        "                      del action_history[j:i+1]\n",
        "                      del state_history[j:i+1]\n",
        "                      del position_history[j:i+1]\n",
        "                      i = j-1\n",
        "                      break\n",
        "            i += 1\n",
        "\n",
        "        return action_history\n",
        "\n",
        "# === Success Experience Buffer ===\n",
        "class SuccessBuffer:\n",
        "    def __init__(self, capacity=100):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, episode_transitions):\n",
        "        filtered = []\n",
        "        for (s, a, r, s_next, done) in episode_transitions:\n",
        "            if not np.array_equal(s, s_next):\n",
        "                filtered.append((s, a, r, s_next, done))\n",
        "        cleaned = remove_redundant_moves(transitions=filtered, px=env.px, py=env.py)\n",
        "        if filtered:\n",
        "            self.buffer.append(cleaned)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if not self.buffer:\n",
        "            return []\n",
        "        # choose randomly from the success buffer\n",
        "        episode = random.choice(self.buffer)\n",
        "        if len(episode) < batch_size:\n",
        "            return episode\n",
        "        else:\n",
        "            return random.sample(episode, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByeWafLzcmpy",
        "outputId": "608ef880-4c0e-4eb9-c777-1ee368b974ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success at Episode 16: steps=27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50: Epsilon=0.905\n",
            "Success at Episode 61: steps=26\n",
            "Episode 100: Epsilon=0.819\n",
            "Episode 150: Epsilon=0.741\n",
            "Success at Episode 162: steps=16\n",
            "Episode 200: Epsilon=0.670\n",
            "Episode 250: Epsilon=0.606\n",
            "Success at Episode 261: steps=23\n",
            "Success at Episode 282: steps=34\n",
            "Episode 300: Epsilon=0.548\n",
            "Total Success: 5\n"
          ]
        }
      ],
      "source": [
        "# === Define a DQN Network ===\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        h, w = input_shape\n",
        "        n_input = h * w\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(), # (H, W) -> (H*W)\n",
        "            nn.Linear(n_input, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 2: # (H, W)\n",
        "            x = x.unsqueeze(0) # batch dimension\n",
        "        return self.net(x)\n",
        "\n",
        "# === Prioritized Experience Replay ===\n",
        "class PrioritizedReplayMemory:\n",
        "    def __init__(self, capacity, alpha=0.7):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.memory = []\n",
        "        self.priorities = []\n",
        "        self.pos = 0\n",
        "\n",
        "    def push(self, transition, td_error=1.0):\n",
        "        p = (abs(td_error) + 1e-5) ** self.alpha\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(transition)\n",
        "            self.priorities.append(p)\n",
        "        else:\n",
        "            self.memory[self.pos] = transition\n",
        "            self.priorities[self.pos] = p\n",
        "            self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.memory) == 0:\n",
        "            raise ValueError(\"Replay memory is empty\")\n",
        "\n",
        "        probs = np.array(self.priorities, dtype=np.float32)\n",
        "        probs_sum = probs.sum()\n",
        "        if probs_sum == 0:\n",
        "            probs = np.ones_like(probs) / len(probs)\n",
        "        else:\n",
        "            probs = probs / probs_sum\n",
        "\n",
        "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
        "        samples = [self.memory[i] for i in indices]\n",
        "\n",
        "        weights = (len(self.memory) * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "        return samples, indices, torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    def update_priorities(self, indices, td_errors):\n",
        "        for idx, td in zip(indices, td_errors):\n",
        "            self.priorities[idx] = (abs(td) + 1e-5) ** self.alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# === DQN Agent ===\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, lr=1e-3, gamma=0.99, epsilon=1.0, eps_min=0.01, eps_decay=0.998):\n",
        "        self.success_buffer = SuccessBuffer(capacity=100)\n",
        "        self.env = env\n",
        "        self.n_actions = env.action_space.n\n",
        "        self.state_shape = env.reset().shape\n",
        "        self.memory = PrioritizedReplayMemory(5000)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_min = eps_min\n",
        "        self.eps_decay = eps_decay\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.policy_net = DQN(self.state_shape, self.n_actions).to(self.device)\n",
        "        self.target_net = DQN(self.state_shape, self.n_actions).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "\n",
        "    # ε-greedy\n",
        "    def select_action(self, state):\n",
        "        state = np.array(state, dtype=np.float32) / 33.0  # normalize\n",
        "        if random.random() < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return int(torch.argmax(q_values).item())\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        reward = max(-10.0, min(10.0, reward)) # reward clipping\n",
        "        self.memory.push((state, action, reward, next_state, done))\n",
        "\n",
        "    def train_step(self, batch_size=64, beta=0.4, episode_num=0):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        # combine sucessful samples\n",
        "        if episode_num >= 200 and len(self.success_buffer) > 0:\n",
        "            success_samples = self.success_buffer.sample(int(batch_size * 0.3))\n",
        "            normal_samples, indices, weights = self.memory.sample(int(batch_size * 0.7), beta)\n",
        "            transitions = success_samples + normal_samples\n",
        "            weights = torch.tensor(np.concatenate([np.ones(len(success_samples)), weights.numpy()]), dtype=torch.float32).unsqueeze(1).to(self.device)\n",
        "        else:\n",
        "            transitions, indices, weights = self.memory.sample(batch_size, beta)\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
        "\n",
        "        states = torch.tensor(np.array(states) / 33.0, dtype=torch.float32).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.tensor(np.array(next_states) / 33.0, dtype=torch.float32).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
        "\n",
        "        weights = weights.unsqueeze(1).to(self.device)\n",
        "\n",
        "        q_values = self.policy_net(states).gather(1, actions)\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].detach().unsqueeze(1)\n",
        "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        td_errors = (target_q_values - q_values).detach().cpu().numpy().flatten()\n",
        "        self.memory.update_priorities(indices, td_errors)\n",
        "\n",
        "        loss = (weights * (q_values - target_q_values).pow(2)).mean()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "# === Training Loop ===\n",
        "agent = DQNAgent(env)\n",
        "num_episodes = 300\n",
        "best_total_reward = -float(\"inf\")\n",
        "best_count = float(\"inf\")\n",
        "best_history = None\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    episode_transitions = []\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    agent.epsilon = max(agent.eps_min, agent.epsilon * agent.eps_decay)\n",
        "    state_history = []\n",
        "    action_history = []\n",
        "\n",
        "    while not done:\n",
        "        action = agent.select_action(state)\n",
        "        result = env.step(action)\n",
        "        if result is None:\n",
        "            continue # ignore invalid moves\n",
        "        state_history.append(state.copy())\n",
        "        action_history.append(action)\n",
        "        next_state, reward, done, _ = result\n",
        "        agent.store_transition(state, action, reward, next_state, done)\n",
        "        agent.train_step(episode_num=episode)\n",
        "        episode_transitions.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    if env._all_tiles_colored():  # Game Clear\n",
        "        agent.success_buffer.push(episode_transitions)\n",
        "        action_count = len(remove_redundant_moves(action_history=copy.deepcopy(action_history), state_history=copy.deepcopy(state_history), px=env.px, py=env.py))\n",
        "        print(f\"Success at Episode {episode+1}: steps={action_count}\")\n",
        "\n",
        "        # save the best episode\n",
        "        if best_history is None or action_count < best_count:\n",
        "            best_count = action_count\n",
        "            best_history = (copy.deepcopy(state_history), copy.deepcopy(action_history))\n",
        "\n",
        "    agent.update_target()\n",
        "\n",
        "    if (episode+1) % 50 == 0:\n",
        "        print(f\"Episode {episode+1}: Epsilon={agent.epsilon:.3f}\")\n",
        "\n",
        "    if episode == num_episodes-1:\n",
        "        print(f\"Total Success: {len(agent.success_buffer)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "pbV-K39CtGyt",
        "outputId": "c4c3e18d-1c8d-4cbe-b9c5-574698bdf09c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replay finished.\n",
            "Steps played: 16, total_reward: 52.881\n",
            "Final scores: {'red': 10, 'blue': 2, 'green': 4}\n",
            "Replay saved as best_replay.gif\n"
          ]
        }
      ],
      "source": [
        "def replay_best_episode(action_history, state_history, env, delay, gif_path=\"replay.gif\"):\n",
        "    if env is None:\n",
        "        raise ValueError(\"env must be provided\")\n",
        "\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "    frames = []\n",
        "\n",
        "    action_history = remove_redundant_moves(action_history=action_history, state_history=state_history, px=env.px, py=env.py)\n",
        "\n",
        "    for action in action_history:\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "\n",
        "        # animation\n",
        "        draw_board(env.data, env.px, env.py)\n",
        "        scores = {\n",
        "            'red': int(np.sum(env.data == 1)),\n",
        "            'blue': int(np.sum(env.data == 2)),\n",
        "            'green': int(np.sum(env.data == 3))\n",
        "        }\n",
        "\n",
        "        # create gif\n",
        "        fig = draw_board(env.data, env.px, env.py, save_fig=True)\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png', bbox_inches='tight', dpi=150)\n",
        "        buf.seek(0)\n",
        "        image = imageio.v3.imread(buf)\n",
        "        frames.append(image)\n",
        "        buf.close()\n",
        "        plt.close(fig)\n",
        "\n",
        "        time.sleep(delay)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(\"Replay finished.\")\n",
        "    print(f\"Steps played: {step}, total_reward: {total_reward:.3f}\")\n",
        "    print(\"Final scores:\", scores)\n",
        "\n",
        "    if frames:\n",
        "        imageio.mimsave(gif_path, frames, duration=delay*1000)\n",
        "        print(f\"Replay saved as {gif_path}\")\n",
        "\n",
        "    return env.data, env.px, env.py, env.player_color, total_reward\n",
        "\n",
        "# === Animate and Save the Most Successful Episode ===\n",
        "if best_history is None:\n",
        "    print(\"There were no sucessful episodes.\")\n",
        "else:\n",
        "    state_history = best_history[0].copy()\n",
        "    action_history = best_history[1].copy()\n",
        "    data, px, py, player_color, total_reward = replay_best_episode(\n",
        "        action_history,\n",
        "        state_history,\n",
        "        env=env,\n",
        "        delay=0.3,\n",
        "        gif_path=\"best_replay.gif\"\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}